<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>command not found:</title>
    <link>/</link>
    <description>Recent content on command not found:</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 25 Dec 2021 10:10:07 +0900</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>サーバー1台で動かす Clos Network Topology</title>
      <link>/posts/learning-clos-network-topology/</link>
      <pubDate>Sat, 25 Dec 2021 10:10:07 +0900</pubDate>
      
      <guid>/posts/learning-clos-network-topology/</guid>
      <description>さくらインターネット Advent Calendar 2021 25日目の記事になります。
JANOG49 Meeting への登壇が決まり、準備を進めています。 Clos Network Topology を運用するために、どのような取り組みをしていますか というタイトルにて、 Clos Network Topology を題材に、運用している皆さんは、学習・教育という切り口にて、どのような取り組みをしているのかを議論したいという発表になっています。
JANOG49 Meeting のニュースレターである 最強プログラムの紹介 に、私の発表についての言及があり、震えています。
今日は、時間の都合にて発表の中には含めることができなかった、 学習のために実装した Clos Network Topology の話を Advent Calender の力を借りて書いていきます。
Clos Network Topology について、何故このようなことをやろうと思ったのか、 学習のために実装した Clos Network Topology にについての順番で書いていきます。 「何故このようなことをやろうと思ったのか」のボリュームが結構あるため、 実装した Clos Network Topology の内容だけを読みたい方は、「実装のモチベーション」くらいから読み進めてください。
Clos Network Topology 私は、 BGP を利用した、スケールアウト可能なデータセンターネットワークと理解しています。 IP CLOS, IP Fabric, Clos Network など様々な呼び方をされているように見えるため、 何と呼んだら、これを他の人に伝えられるのかがよくわかっていません。 どなたか詳しい方がいましたら、教えていもらえるとうれしいです。
今回は、 RFC7938 に出てくる Clos Network Topology という呼び方で話を進めていきます。
https://tools.ietf.org/html/rfc7938
学習のモチベーション いくつもの条件が重なり、学習のための Clos Netowork Topology 実装に繋がっているというところを書いていきます。</description>
    </item>
    
    <item>
      <title>Grafana Loki を使ったログ監視</title>
      <link>/posts/log-monitoring-using-grafana-loki/</link>
      <pubDate>Fri, 24 Dec 2021 10:07:44 +0900</pubDate>
      
      <guid>/posts/log-monitoring-using-grafana-loki/</guid>
      <description>さくらインターネット Advent Calendar 2021 24日目の記事になります。
前回は、 Grafana Loki を使ったログ管理基盤 について書きました。
今回は、 Grafana Loki を使いログを監視したい場合、どのような構成となるのか、 Graylog でのログ監視を考えてみる と比べて何がうれしいのか、 どういう場面でログ監視を使うことになりそうかについて書いていきます。
Grafana Loki を使ったログ管理基盤 と同じバージョンで話しを進めていきます。
Grafana Loki でのログ監視 Grafana Loki では、 Ruler というコンポーネントを使いログ監視を実現します。
Ruler を使えば、 ログ監視が実現できるということはわかりましたが、 Grafana Loki は、クラウドのストレージからログをロードします。 アラートを評価するインターバルは、任意の値を設定できますが、デフォルトは、1分となっています。
例えば、アラートの設定が、 100 あった場合に、それらを評価するためにログのロードが発生すると、 全てのアラートの評価が終わるまでに、1分以上掛かってしまうのではないかという疑問が浮かびました。
Ruler のドキュメントを眺めてみると、 Ruler を複数台動かしアラートの評価をシャーディングできるということがわかりました。
https://grafana.com/docs/loki/v2.2.1/alerting/#scheduling-and-best-practices
Ruler の情報は Consul の KVS に登録され、 それぞれの Ruler は、 Consul の KVS に登録された Ruler の情報を参照し、 他の Ruler と連携することで、アラートの評価をシャーディングします。
シャーディングを有効化した5台の Ruler を起動し、2つのアラートを設定し、 Ruler の挙動を確認してみたところ、5台の Ruler で、2つのアラートがそれぞれ評価されるのではなく、 5台の Ruler のうち、2台の Ruler でアラートが評価されていることを Ruler のログから観測できました。</description>
    </item>
    
    <item>
      <title>Grafana Loki を使ったログ管理基盤</title>
      <link>/posts/log-management-using-grafana-loki/</link>
      <pubDate>Sun, 19 Dec 2021 13:35:41 +0900</pubDate>
      
      <guid>/posts/log-management-using-grafana-loki/</guid>
      <description>さくらインターネット Advent Calendar 2021 19日目の記事になります。
現在、「さくらの専用サーバ PHY」のログ管理基盤にて、 Grafana Loki を利用しています。
今回は、 Graylog を使ったログ管理基盤 を運用し得られた知見、 「さくらの専用サーバ PHY」での Grafana Loki のアーキテクチャと Graylog の運用と異なる点はどうであるかについて書いていきます。
Graylog を運用し得られた知見 Graylog は、メンテナンスなどの計画された作業以外では、 一度も停止することなく「さくらの専用サーバ PHY」のログ基盤を支えてくれました。 Graylog を2年半運用し得られた知見のいくつかを紹介することで、 Graylog を使っていない方々に運用のイメージを持ってもらえたらと思っています。
Elasticsearch での型によりログが保存されない ログの保存先である Elasticsearch は、 Key, Value という形でログを保持しており、 Value には、 integer, string などの型が付いています。 型は、自動でマッピングされます。
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html
例えば、 status という Kye を持ったログの Value の型が string だった場合に、 status: ok のログを出力するアプリケーションのログは、 Elasticsearch に保存されますが、 status: 200 のログを出力するアプリケーションのログは、 Elasticsearch の型とは異なるため、 Graylog が下記のエラーを吐き、 Elasticsearch にはログが保存されないということが起きます。
2020-06-03 17:15:56,386 WARN : org.</description>
    </item>
    
    <item>
      <title>Graylog でのログ監視を考えてみる</title>
      <link>/posts/log-monitoring-using-graylog/</link>
      <pubDate>Sat, 18 Dec 2021 10:09:23 +0900</pubDate>
      
      <guid>/posts/log-monitoring-using-graylog/</guid>
      <description>さくらインターネット Advent Calendar 2021 18日目の記事になります。
Graylog を使ったログ管理基盤 では、「さくらの専用サーバ PHY」にて過去に利用していたログ管理の仕組みについて説明しました。
ログには、何かあったときに調査するための側面と、 このログが出力されたら対応が必要な監視の側面があると思います。
今回は、実際には Grafana Loki にて実現したため使うことはありませんでしたが、 「さくらの専用サーバ PHY」にて、 Graylog を使ったログ監視について考えていたとを書いていきます。
監視について考えていたこと 「さくらの専用サーバ PHY」の監視について、だたアラートがどこかに通知できればよいと考えていたわけではないことを書いていきます。
アラート通知のインタフェースには Alertmanager を使う 今までに係わってきたシステムでは、アラートを通知してくるものが、 Nagios だったり、 Cacti だったり、単体のアプリケーションだったりと、 それぞれが独自に通知する仕組みを持っていると、それぞれの仕組みを見なければならず、運用の負荷た高いなと感じていました。
「さくらの専用サーバ PHY」では、 VictoriaMetrics を使ったメトリクス管理基盤 のように、 メトリクス監視の通知に Alertmanager を使っています。
ログ監視に対して、独自に通知する仕組みを作り込むのではなく、アラート通知のインタフェースを Alertmanager に統一することで、 アラートについては、 Alertmanager を見ればよい、 アラート通知を停止したり、抑止したりといったオペレーションを統一していきたいと考えていました。
メトリクス監視の Alertmanager のルーティング設定を使いたい メトリクス監視では、 ALERTING RULES のように PromQL の式に加えて label, annotation を設定します。 メトリクス監視にて設定された label などを設定し、 Alertmanager では、受け取ったアラートに付与されている label から、アラートをどこに通知するかのルーティングを設定します。
ログ監視のインタフェースを Alertmanager に統一できたとして、 ルーティングの設定がメトリクス監視と異なるといった状況になると、 Alertmanager のルーティング設定が肥大化し、それはそれで運用の負荷に跳ね返ってくるのではないかと思っていました。</description>
    </item>
    
    <item>
      <title>Graylog を使ったログ管理基盤</title>
      <link>/posts/log-management-in-graylog/</link>
      <pubDate>Fri, 17 Dec 2021 08:00:29 +0900</pubDate>
      
      <guid>/posts/log-management-in-graylog/</guid>
      <description>さくらインターネット Advent Calendar 2021 17日目の記事になります。
syslog を使ったログ管理について考察してみる にて、 syslog server にファイルでログを管理するという構成をとった場合に、どうなりそうかについて考えてみました。
「さくらの専用サーバ PHY」では、 syslog を使ったログ管理について考察してみる のようなログ管理はしておらず、 Graylog を使いログを管理していました。 Graylog でのログ管理を2年半ほど運用し、現在は Grafana Loki に切り替わっています。
今回は、2年半とてもお世話になった Graylog を使うと、 syslog を使ったログ管理について考察してみる と同じ観点で、 どのようなログ管理を実現できるのかについて書いていきます。
アーキテクチャ Graylog は、単体で動くものではなく、 設定情報を MongoDB で管理し、 ログを Elasticsearch で管理するアーキテクチャとなっています。 Graylog は、ログ受信、ログ検索の Web UI を提供します。
https://docs.graylog.org/docs/architecture
Graylog Graylog は master, slave の構成をとっています。 master にて Graylog を設定すると、設定情報は MongoDB に保存されます。 slave は MongoDB に保存された情報を参照し、動作します。
https://docs.graylog.org/docs/manual-setup
MongoDB Graylog の設定や Elasticsearch のインデックスなどの情報が保存されています。 を構成します。
Graylog で利用する MongoDB の構成については、公式ドキュメントや他の記事にお任せします。</description>
    </item>
    
    <item>
      <title>syslog を使ったログ管理について考察してみる</title>
      <link>/posts/log-management-in-syslog/</link>
      <pubDate>Tue, 14 Dec 2021 20:20:32 +0900</pubDate>
      
      <guid>/posts/log-management-in-syslog/</guid>
      <description>さくらインターネット Advent Calendar 2021 14日目の記事になります。
今日から、ログ管理について考えていることを何回かに分けて書いていきたいと思っています。
「さくらの専用サーバ PHY」では Grafana Loki にて、ログ管理・監視の基盤を構築しています。 Grafana Loki の前には、 Graylog を使いログを管理していました。
Grafana Loki がイケているから使っているのではなく、 syslog を使ったログ管理を運用した場合どのようなことが考えられるか、 syslog ではなく Graylog を使うと何がうれしくて、 Grafana Loki を使うと Graylog よりも何がうれしかったのかという流れで書いていく予定です。
ログ管理となると一番馴染があるのは、 syslog なのではないかと思っています。 今回は、 syslog を使ったログ管理を運用した場合、どのようなことが考えられるかについて書いてみます。
syslog でのログ管理 syslog のエージェントからログが転送され、 syslog サーバーは受信したログを自身のストレージにファイルで書き出すという構成を仮定した場合に、 下記の観点にてログ管理がどのようになるのか考察してみます。
ログ出力 冗長化 ストレージ拡張 ログ参照 ログのローテート スケールアウト ログ転送・保存の構成としては、下記の2つのパターンがあるのではないかと思います。
転送先の Syslog Server を複数設定した構成 アプリケーションは、ログ出力に syslog を使い、 syslog エージェントは、2つの syslog server にログを転送し、 2つの syslog server には同じログが保存されているという構成になります。
ロードバランサを Syslog Server のエンドポイントに設定した構成 アプリケーションは、ログ出力に syslog を使い、 syslog エージェントは、ロードバランサのエンドポイントにログを転送し、 2台ある syslog server のどちらか一方に syslog エージェントから転送されたログが保存されているという構成になります。</description>
    </item>
    
    <item>
      <title>VictoriaMetrics を使ったメトリクス管理基盤</title>
      <link>/posts/victoriametrics-monitoring/</link>
      <pubDate>Mon, 13 Dec 2021 09:23:39 +0900</pubDate>
      
      <guid>/posts/victoriametrics-monitoring/</guid>
      <description>さくらインターネット Advent Calendar 2021 13日目の記事になります。
今日は、「さくらの専用サーバ PHY」にてメトリクス管理の基盤に、 VictoriaMetrics を使っているという話を書いていきます。
VictoriaMetrics のアーキテクチャやコンポーネントについての詳しい説明は、 公式ドキュメント や、 他の記事にお任せし、「さくらの専用サーバ PHY」のメトリクス管理の基盤が、どのような構成ではじまり、 現在どうなっているのか、これからどうしていこうと考えているのかについて書いていきます。
2020年7月時点の構成 「さくらの専用サーバ PHY」提供開始時のメトリクス管理・監視基盤の構成になります。
Promxy を使ったアーキテクチャ より、 Promxy にリクエストすることで、 Prometheus のメトリクスにアクセスする構成をとっています。
30日ぶんのトラフィックグラフをユーザーに提供するため、 Prometheus には30日ぶんのメトリクスを保存するようにしています。
Prometheus には remote write という機能があり、 取得したメトリクスを別のストレージに転送することが可能になっています。 長期的なメトリクスを保存する場合は、 Prometheus に保存するのではなく、 Prometheus の remote write を使い、 長期保存のためのストレージを使う構成をとることを推奨しています。
https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage
このタイミングでは、 サービスを支えるシステム基盤のメトリクスをどのように管理するのか、 ユーザーに提供している機器をどのようにメトリクス管理するのか、 複数台ある Prometheus のメトリクスをどのように参照するのか、 というようなところを検証、実装していたところで、サービスの提供開始となりました。
長期的なメトリクスを保存するためのストレージを検証、導入する時間を確保できなかったため、 Promxy をエンドポイントに Prometheus のメトリクスにアクセスする構成をとりました。
利用者の増加とともに Prometheus は追加されていきます。 Promxy をエンドポイントに Prometheus のメトリクスにアクセスする構成で、 どこまでスケールできるのか未知のまま運用している状態でした。
Promxy が、 Grafana へのエンドポイント、ユーザーのトラフィックグラフへのエンドポイント、 アラート発砲と3つのサービスを提供しており、ダウンした場合の影響が大きいところも自分の中で引っ掛かっていました。</description>
    </item>
    
    <item>
      <title>Prometheus の Exporter を自分なりに分類してみた</title>
      <link>/posts/classify-prometheus-exporter/</link>
      <pubDate>Sun, 12 Dec 2021 09:07:00 +0900</pubDate>
      
      <guid>/posts/classify-prometheus-exporter/</guid>
      <description>さくらインターネット Advent Calendar 2021 12日目の記事になります。
Prometheus を使うと何がうれしいのか自分なりに考えてみた にて、 Prometheus ではどのようなモニタリングを実現でき、従来のモニタリングの仕組みと比べてのうれしい部分について考えてみました。
Prometheus では、 Exporter を使い対象のメトリクスを取得するということを説明しました。 Exporter がどのようなものであるかを説明したあと、 すぐに ○○ Exporter の使い方というような流れであったり、 特定の Exporter の使い方という記事はよく見掛けます。
対象のメトリクスを取得するために、 どのような Exporter の構成があるのかを説明した記事は見掛けたことが無く、 Prometheus と Exporter の結び付きをイメージするためには必要なのではないかと常日頃から思っていたので、 そこに対する自分の考えを書いてみます。
Exporter の構成 対象のメトリクスを取得するためにとる Exporter の構成は、下記の3つになるのではないかと考えています。 Sidecar exporter, Native exporter, Proxy exporter という名前は、 説明のために、私が作ったものであり、 Prometheus で使われている一般的な用語ではありません。 Prometheus でメトリクスを管理するためには、 Prometheus の保存できる形式でメトリクスを生成し、 HTTP のエンドポイントを公開することが必須であるという前提のもと、 これらについて、どのような構成をとり、どう考え名前を付けたのかについて説明します。
Native exporter Apache のメトリクスを Prometheus で管理したいとなったときに、 Apache 自体は、 Exporter を持っていないため、 そのままでは、 Apache のメトリクスを Prometheus で管理できません。
Prometheus が保存できる形式でメトリクスを返すことができる機能を、 ソフトウェア自体に組み込んでいる Exporter を Native exporter と呼んでみました。</description>
    </item>
    
    <item>
      <title>Prometheus を使うと何がうれしいのか自分なりに考えてみた</title>
      <link>/posts/motivation-use-prometheus/</link>
      <pubDate>Sat, 11 Dec 2021 10:05:45 +0900</pubDate>
      
      <guid>/posts/motivation-use-prometheus/</guid>
      <description>さくらインターネット Advent Calendar 2021 11日目の記事になります。
Prometheus 以前のモニタリングの仕組みを考察してみる では、 Nagios, Cacti を例に Prometheus 以前のモニタリングの仕組みはどのようになっているかを考察し、 Nagios, Cacti で「さくらの専用サーバ PHY」のモニタリングを実装するとどうなりそうであるかを考え、 「けっこうアプリケーションを作ら作らなければならないな」ということが見えてきました。
今回は、 Prometheus を使うと、どのようなモニタリングの基盤を構成可能であるかを考察し、 Prometheus を使うと何がうれしいのかを考えてみます。
用語 Prometheus の話をするにあたり、知っておかなければ話を進められない用語について説明します。
メトリクス Prometheus は、下記のような形式にて対象の情報を取得し、メトリクスを呼ばれています。
メモリ利用量のメトリクスの例
# HELP node_memory_MemAvailable_bytes Memory information field MemAvailable_bytes. # TYPE node_memory_MemAvailable_bytes gauge node_memory_MemAvailable_bytes 6.2036496384e+10 Exporter エクスポーターと呼びます。 Prometheus にメトリクスを保存するためには、 Prometheus が保存できる形式のメトリクスを HTTP として公開する必要があります。 この機能を持ったアプリケーションを Exporter と呼んでいます。 Prometheus にメトリクスを保存するためには Exporter が必須となります。
Prometheus でのモニタリング Prometheus 以前のモニタリングの仕組みを考察してみる では、 Nagios, Cacti にて「さくらの専用サーバ PHY」のモニタリングを考察しました。 今回は、 Prometheus を使った場合の「さくらの専用サーバ PHY」のモニタリングを考察します。</description>
    </item>
    
    <item>
      <title>Prometheus 以前のモニタリングの仕組みを考察してみる</title>
      <link>/posts/monitoring-mechanism-before-prometheus/</link>
      <pubDate>Thu, 09 Dec 2021 13:32:10 +0900</pubDate>
      
      <guid>/posts/monitoring-mechanism-before-prometheus/</guid>
      <description>さくらインターネット Advent Calendar 2021 9日目の記事になります。
Prometheus を知っているでしょうか。 近年、モニタリングを賑わせているソフトウェアになります。
Prometheus についての内容、 Prometheus の設定方法、どのような Exporter があり、 どのように使うのかという記事は、インターネット上でたくさん目にしましたが、 従来のモニタリングの仕組みと比べ、 Prometheus がどのような点で異なっており、 その異なる部分がモニタリングにどのように生きてくるのかということを書いた記事は見掛けたことはありません。
自分なりに Prometheus 以前のモニタリングの仕組みはどうであるかを考察し、 Prometheus がイケてるという理由で使われているのではなく、 従来のモニタリングの仕組みと比べて、 Prometheus を使うと何がうれしいのかということを書いていけたらと思っています。
今回は、 Prometheus 以前のモニタリングの仕組みはどうであるか考察し、「さくらの専用サーバ PHY」に適用するとどうなりそうかについて考えてみます。
従来のモニタリングの仕組み モニタリングには、システムの異常に気が付き対応できるようにするための死活監視、 アラートが上がったときなどにシステムのリソースなどを時系列で確認するためのシステムが存在しているはずです。
下記の観点から従来のモニタリングの仕組みがどうであるかについて考察してみます。
対象の追加・削除 ストレージ インタフェース スケールアウト Nagios , Cacti を従来のモニタリングの仕組みとして考察します。
Nagios でのモニタリング Cacti でのモニタリング 対象の追加・削除 モニタリングしたい対象のサーバーが1台あったと仮定して、 モニタリングのため、死活監視を Nagios に登録し、グラフを Cacti に登録するなど、 それぞれのシステムごとにモニタリングの対象を登録していかなければなりません。
Nagios, Cacti ともに IPAM などと連携し対象を登録していくという仕組みはなく、 対象の追加・削除は手動で行うことになります。 「対象の追加・削除を自動化したい」となったときには、そのための仕組みを開発する必要がありそうです。
ストレージ 死活監視のデータは Nagios に保存され、 グラフのためのデータは Cacti に保存されるため、 システムごとにストレージが異っています。</description>
    </item>
    
    <item>
      <title>コンテナオーケストレーションにて必要になってくるもの</title>
      <link>/posts/technologies-required-for-container-orchestration/</link>
      <pubDate>Wed, 08 Dec 2021 08:05:12 +0900</pubDate>
      
      <guid>/posts/technologies-required-for-container-orchestration/</guid>
      <description>さくらインターネット Advent Calendar 2021 8日目の記事になります。
コンテナオーケストレーションを使うモチベーションを自分なりに考えてみた にて、 Docker を導入するだけでは解決しなかった課題を Nomad と組み合わせることにより解決できそうであるということを説明しました。
今回は、 Docker と Nomad を組み合わせるだけでは解決できていないことに対して必要になってくる技術について、 従来のシステム連携がどのようになっているかという話と絡めながら説明します。
Nomad を使うことで、 Docker を実行できるサーバーを沢山並べてアプリケーションを実行できるようになり、 アプリケーションとサーバーが密に結合しなくなりました。 Docker で動くアプリケーションが、どの Nomad Client で起動するのかは、 Nomad Server が管理しているため、人間にはコントロールできません。
アプリケーションは、他のアプリケーションと連携するというパターンが大部分を占めるのではないかと思います。 どこかの Nomad Client でアプリケーションが起動しているという状態で、 どのようにしてアプリケーションにアクセスすればよいでしょうか。
従来のシステム連携 Nomad Client で起動しているアプリケーションにどのようにアクセスするのかの手段を説明する前に、 従来のシステム連携がどうであるかについて、以下の観点から考えてみます。
アプリケーションへの接続 依存関係 アプリケーションが連携している例になります。
アプリケーションへの接続 サーバーに割り当たっている IP アドレスからアプリケーションに接続するというやり方が考えられます。
IP アドレス以外の方法としては、 DNS を使いサーバーに割り当たっている IP アドレスに名前を付け、 アプリケーションに接続するというやり方が考えられます。
どちらのやり方の場合も、サーバーに割り当てられている IP アドレス、 名前からアプリケーションに接続するという構図になっているのではないかと思います。
システム連携の主体は、アプリケーションではなく、サーバーにあるように見えます。
依存関係 IP アドレス、 DNS などアプリケーションへの接続は、サーバーに紐付いている情報に依存しており、 アプリケーションに接続するというよりは、あるサーバー上で動いているアプリケーションに接続するという依存関係になっているように見えます。
従来のやり方でアプリケーションに接続できるか どれからの Nomad Client でアプリケーションが起動しているという状況にて、 従来のシステム連携のやり方が通用するのかについて考えてみます。</description>
    </item>
    
    <item>
      <title>コンテナオーケストレーションを使うモチベーションを自分なりに考えてみた</title>
      <link>/posts/motivation-using-container-orchestration/</link>
      <pubDate>Mon, 06 Dec 2021 10:26:12 +0900</pubDate>
      
      <guid>/posts/motivation-using-container-orchestration/</guid>
      <description>さくらインターネット Advent Calendar 2021 6日目の記事になります。
Docker でのアプリケーション実行環境を提供した状況を想像してみる にて、 Docker を導入しただけで、万事解決とはならなそうであるということを書きました。
今回は、 Docker の導入だけでは解決できないことに対して、どのようなことができればよいのかということを考え、それらを解決する具体的な手段について書いていきます。
Docker だけでは解決できていないこと 下記のようなことができれば、アプリケーション開発者はサーバーのことを気にせず、 サーバー管理者はアプリケーションのことを気にしない仕組みになるのではないかと考えました。
「 Docker を実行できる環境があれば、どこでもアプリケーションを実行できるが、サーバーで Docker を実行できるだけでは、サーバーに付けた名前や IP アドレスで、アプリケーションに接続するという構図から抜け出せない」
Docker を実行できるサーバーを沢山用意して、その中のどれかでアプリケーションが実行されるということができれば、 アプリケーションとサーバーが密に結合するという状態を解消できそうです。
「サーバー管理者の裁量でサーバーをコントロールできる部分が少なそう」
Docker を実行できるサーバーを沢山用意して、 その中のどれかでアプリケーションが実行されるということができるかつ、 メンテナンスしたいサーバーでは Docker コンテナを実行できないようにすれば、 アプリケーション開発者は、サーバーの停止を意識することがなくアプリケーションを実行でき、 サーバー管理者は、サーバーを任意のタイミングで停止できるようになりそうです。
「 Docker で動くアプリケーションをデプロイする統一されたインタフェースが無い」
定義ファイルを書いて実行すれば Docker で動くアプリケーションがデプロイされる仕組みがあれば、 構成管理もできてよさそうです。 アプリケーション開発者は、定義ファイルを書いて実行することでアプリケーションを実行し、 サーバー管理者は、 Ansible でサーバーを構成管理することで、 アプリケーションとサーバー管理の手段を分離できそうです。
Nomad を使えば実現できる Docker だけでは解決できていないことに対して、どのようなことができればよいかを考えてみました。 これらは Docker に Nomad というソフトウェアを組み合わせることで、実現できます。
「Nomad とは」というお話は、別の記事や公式ドキュメントに任せ、 Nomad と Docker を組み合わせることで何ができるのかについて、書いていきます。
https://www.nomadproject.io
アーキテクチャ https://learn.hashicorp.com/tutorials/nomad/production-reference-architecture-vm-with-consul#reference-diagram
緑色の Nomad と書かれているところだけを説明していきます。 ( 赤色の Consul と書かれているところは、今回は気にしないでください。 )</description>
    </item>
    
    <item>
      <title>Docker でのアプリケーション実行環境を提供した状況を想像してみる</title>
      <link>/posts/imagine-situation-using-docker/</link>
      <pubDate>Sun, 05 Dec 2021 13:45:07 +0900</pubDate>
      
      <guid>/posts/imagine-situation-using-docker/</guid>
      <description>さくらインターネット Advent Calendar 2021 5日目の記事になります。
Docker を使うと何がうれしいのか自分なりに考えてみた にて、 Docker を使うと、どういった面でうれしいことがあるのかについて書きました。
Docker を使うと何がうれしいのかということは説明できたかと思いますが、 Docker を提供すれば、「万事解決めでたしめでたし」となるのかというところを考えていきたいと思います。
サーバー管理者になって考えてみる 今回は、サーバー管理者として Docker でのアプリケーション実行環境をアプリケーション開発者に提供したと仮定し、 下記の観点にて、どのようなことが考えられそうかについて、説明していきます。
依存関係 移植性 デプロイ 下記、 Docker によるバッチサーバー、アプリケーションサーバーを提供したとして、何が起きそうかについて想像力を働かせてみます。
バッチサーバー アプリケーションサーバー 依存関係 サーバーの中だけを見ると Docker が動く環境さえ用意すればアプリケーションはどこでも実行できる状態になっています。
アプリケーションサーバーは、どこかのアプリケーションからリクエストを受ける、システム連携の側面が強いのではないかと思います。
システム連携を考えたときに、どのようにアプリケーションに到達するのか考えてみます。 このサーバー構成では、サーバーに付けた名前 ( DNS ) や IP アドレスでアプリケーションに到達することになるでしょう。
このサーバーに付けた名前や IP アドレスで、アプリケーションに接続するというシステム連携になっており、 アプリケーションがサーバーとの依存関係を断てていないという状況が見えてきます。
Docker だけでは、アプリケーションがサーバーに強く依存している状況を打破することが難しそうだなということが考えられます。
移植性 一度、サーバー構築してしまえば、そこからはメンテナンスフリーとはならないことを皆さんよくご存知かと思います。 OS 、ミドルウェア、アプリケーションを更新していきながらサービスを維持していくことになるはずです。 Docker も Linux などのサーバー上で動くため、これらの更新からは逃れられません。
バッチサーバーの OS にパッチを当て再起動するという状況は容易に想像できるかと思います。 アプリケーション開発者にバッチサーバーを再起動しますと調整するコストが発生しそうです。
バッチサーバー、アプリケーションサーバーの OS を更新するという作業も容易に想像できるかと思います。 アプリケーション開発者と OS を更新するための計画を調整するコストが発生しそうです。
サーバーの中だけを見ると Docker が動く環境さえ用意すればアプリケーションはどこでも実行できる状態になっていますが、 アプリケーションとサーバーが密に結合しており、サーバー管理者の裁量でサーバーをコントロールできる部分が少なそうに思います。
「Docker が動く環境さえ用意すればアプリケーションはどこでも実行できる」が、アプリケーションとサーバーが密に結合しているので、サーバーを更新していくという側面では、 Docker を使っていなかった頃と変わっておらず、移植性を生かしきれていないように見えます。</description>
    </item>
    
    <item>
      <title>Docker を使うと何がうれしいのか自分なりに考えてみた</title>
      <link>/posts/benefits-of-using-docker/</link>
      <pubDate>Sat, 04 Dec 2021 08:37:56 +0900</pubDate>
      
      <guid>/posts/benefits-of-using-docker/</guid>
      <description>さくらインターネット Advent Calendar 2021 4日目の記事になります。
Docker が何者なのか、 Docker の使い方などの情報はインターネットに沢山公開されていますが、 「 Docker を使うと何がうれしいのか」という情報が目に入ってきたことはないので、 サーバー管理者、アプリケーション開発者がいる環境で、 Docker を使うと何がうれしいのかということを自分なりに考え、テキストに起こしてみました。
きっかけ 文学フリマに出店したときに、「インフラエンジニアやってます。仕事は仮想サーバーですが、 Docker に興味はあって調べてはみたのですが、 Docker を使うと何がうれしいのかということをイメージするには至れなかったです。仮想サーバーから Docker になると何がうれしいのかということをイメージできるような情報があると、 このモヤモヤが晴れるのではないかと思っています。」という話を参加者の一人からされました。
確かに、自分もそのような情報が目に入ってきたことはりませんでした。 社内で「 Docker を使うと何がうれしいのか」というテキストを自分で書いて、チーム内に展開しており、 会社の情報は入っていないので、これを公開すればよいかもしれないなと考え、今回、この記事を書くに至っています。
アプリケーションとランタイム これから先の説明に利用する、アプリケーションとランライムという言葉を定義しておきます。
アプリケーション: 目的の処理を実行するもの ランタイム: アプリケーションを実行するときに必要なもの シェルスクリプトで、集計するスクリプトを作成したと仮定した場合に、 作成したシェルスクリプトはアプリケーションで、 シェルスクリプトの中で sed, awk などを使っていれば、 シェルスクリプトというアプリケーションの実行に必要なものということで、 sed, awk はランタイムとなります。
想定読者 下記に該当する方に自分ごととしてイメージしながら読んでもらえるのではないかと考えています。
アプリケーションを実行するための環境を Linux でサーバーを構築している サーバーを管理する人とアプリケーションを開発する人が分かれており、アプリケーションを実行するための環境を Linux で構築するサーバーを管理する側の人である バッチサーバーなるサーバーを用意し、そこで cron を使い様々なバッチを実行している Docker は何をするものなのか インターネットに沢山の情報があるため、 Docker の詳細な内容は、そちらにお任せします。
アプリケーションを実行する手段 アプリケーションを Docker の提供している方法でパッケージングする Docker の提供している方法でパッケージングした(された)アプリケーションを Docker の提供している方法で実行する Docker の提供している方法でパッケージングした(された)アプリケーションを実行するため、サーバーに Docker がインストールする 社内で動かしている Redmine のアクティビティを定期的に Slack へ通知するアプリケーションを例に、 サーバー上でのアプリケーション、ランタイムの構成を図にしました。</description>
    </item>
    
    <item>
      <title>Promxy を使ったアーキテクチャ</title>
      <link>/posts/promxy-architecture/</link>
      <pubDate>Fri, 11 Dec 2020 07:18:38 +0900</pubDate>
      
      <guid>/posts/promxy-architecture/</guid>
      <description>さくらインターネット Advent Calendar 2020 11日目の記事になります。
「さくらの専用サーバ PHY」にてメトリクス保存・公開の基盤を担当している者です。 アプリケーション、サーバー、ネットワーク機器などのメトリクスを保存・公開する基盤として Prometheus を利用しています。 Prometheus にあるメトリクスを使い、 ユーザーにトラフィックグラフを提供したり、 自分達がシステムの異常に気が付くための監視を構成しています。
Prometheus のメトリクスを利用するにあたり、抱えていた2つの課題を Promxy を利用することによって解決することができ、 そこに至るまで、どのように考え、 どのようなアーキテクチャをとったのか、これからどうしていこうと考えているのかについて書きました。
ユーザーにトラフィックグラフを提供する 「さくらの専用サーバ PHY」のコントロールパネルにてトラフィックグラフを提供しており、 値は Prometheus に保存されているメトリクスを使っています。 ユーザーにトラフィックグラフのサービスを提供するにあたり、 インフラ側の可用性を上げるための構成を考え、実装する必要がありました。
Prometheus での高可用性 2台の Prometheus を動かし同じメトリクスを取得させるというのが公式の見解となっています。
https://prometheus.io/docs/introduction/faq/#can-prometheus-be-made-highly-available
ロードバランサを使った構成 Prometheus 公式の構成で、ロードバランサをエンドポイントにしてバックエンドに2台の Prometheus がいるという構成をとったとして、どうなるかを考えてみました。
この構成では、どちらからの Prometheus が一定時間落ちて復帰した場合、 保存しているメトリクスの対称性が失われてしまい、 エンドポイントにリクエストするごとに返ってくるメトリクスの結果が異なってしまうという状態がが発生するなと考え、採用を見送りました。
ロードバランサと Promxy を組み合わせた構成 Promxy は、メトリクスを Aggregate する機能を持っています。 Prometheus のセットが同じタイミングで落ちない限りは、 対称性の失われた Prometheus のセットに対して、 欠損することなくメトリクスを返すことができるため、 Promxy を使ったこちらの構成を採用しました。
アラートルールの管理 サービスの利用者が増えれば、利用される機器も増え、取得するメトリクスも線形に増加していきます。 1台の Prometheus 取得できるメトリクスには限界があり、 「さくらの専用サーバ PHY」で扱っている全てのメトリクスは管理できません。 Prometheus を複数台運用することが前提となります。 Prometheus のメトリクスを使ったアラート発報のためのアラートルールをどのように管理していくのかという問題に当たりました。</description>
    </item>
    
    <item>
      <title>cowbuilder</title>
      <link>/posts/cowbuilder/</link>
      <pubDate>Wed, 30 Sep 2020 00:27:00 +0900</pubDate>
      
      <guid>/posts/cowbuilder/</guid>
      <description>仕事で様々なバージョンの Debian パッケージを作らなければならなかったとき、 環境を用意するコストがとても低くて重宝した。
% apt-get install cowbuilder chroot 環境の作成
% cowbuilder --create --distribution buster --architecture amd64 --basepath /var/cache/pbuilder/buster64.cow chroot 環境へのログイン
# cowbuilder --login --basepath /var/cache/pbuilder/buster64.cow &amp;ndash;save オプションを付けると chroot 環境への変更を保存できる。
% cowbuilder --login --save --basepath /var/cache/pbuilder/buster64.cow bind mount
% cowbuilder --login --save --bindmount /home/workdir --basepath /var/cache/pbuilder/buster64.cow 下記を実行し、作成した chroot 環境をまとめて更新していた。
% for a in /var/cache/pbuilder/*.cow; do cowbuilder --update --basepath $a; done </description>
    </item>
    
    <item>
      <title>LXC</title>
      <link>/posts/lxc/</link>
      <pubDate>Tue, 29 Sep 2020 17:50:53 +0900</pubDate>
      
      <guid>/posts/lxc/</guid>
      <description>$ uname -srv Linux 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) $ cat /etc/debian_version 10.4 依存関係でいろいろ入ってしまうけれども iptables, dnsmasq, bridge-utils それぞれを操作するのが面倒なので、 virsh (1) で全てよしなにやってもらうことにした。
% apt-get install libvirt-clients libvirt-daemon-system dnsmasq bridge-utils % virsh net-list Name State Autostart Persistent -------------------------------------------- default active no yes $ ip addr show virbr0 3: virbr0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 52:54:00:75:2f:a9 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever LXC インストール</description>
    </item>
    
    <item>
      <title>chroot &#43; OverlayFS</title>
      <link>/posts/chroot-overlayfs/</link>
      <pubDate>Mon, 28 Sep 2020 22:59:31 +0900</pubDate>
      
      <guid>/posts/chroot-overlayfs/</guid>
      <description>出番がないかもしれないが、 OverlayFS で chroot 環境をクリーンに保つをやってみた。
$ uname -srv Linux 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) $ cat /etc/debian_version 10.4 % debootstrap buster ./buster64 http://ftp.jp.debian.org/debian OverlayFS でのマウントについては mount (8) を参照
% mkdir upper workdir % mount -t overlay overlay -o lowerdir=./buster64,upperdir=./upper,workdir=./workdir ./buster64 % df ファイルシス 1K-ブロック 使用 使用可 使用% マウント位置 udev 2005984 0 2005984 0% /dev tmpfs 404160 5436 398724 2% /run /dev/vda3 99016264 2958444 91008396 4% / tmpfs 2020792 0 2020792 0% /dev/shm tmpfs 5120 0 5120 0% /run/lock tmpfs 2020792 0 2020792 0% /sys/fs/cgroup overlay 99016264 2958444 91008396 4% /root/buster64 % mount --rbind /dev .</description>
    </item>
    
    <item>
      <title>OpenConnect</title>
      <link>/posts/openconnect/</link>
      <pubDate>Thu, 04 Apr 2019 01:27:45 +0900</pubDate>
      
      <guid>/posts/openconnect/</guid>
      <description>GlobalProtect で VPN 接続する機会があり、 Mac も Windows も持っていないのでどうしたものかと調べていたところ、 OpenConnect というソフトウェアで GlobalProtect に VPN 接続することができることを知ったので記録として残しておく。 OpenConnect の開発者と Debian パッケージのメンテナーありがとう。
openconnect
https://packages.debian.org/ja/sid/openconnect
環境 $ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ uname -a Linux koyomi 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15) x86_64 GNU/Linux fchu@koyomi:~$ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ apt-cache show openconnect Package: openconnect Version: 8.</description>
    </item>
    
    <item>
      <title>HUGO</title>
      <link>/posts/hugo/</link>
      <pubDate>Sat, 19 Jan 2019 21:47:42 +0900</pubDate>
      
      <guid>/posts/hugo/</guid>
      <description>debian-devel-changes のメーリングリストを眺めていたら、 HUGO という Site Generator を見付けた。 数年前にブログ環境を消失してから、ブログを書く気力が無くなってしまっていたが、HUGO を使い Github Pages にブログを書くことを再開しようと思う。
debian-devel-changes
https://lists.debian.org/debian-devel-changes/
HUGO
https://gohugo.io/
HUGO の Debian パッケージ
https://packages.debian.org/sid/hugo
実行環境 $ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ uname -a Linux koyomi 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15) x86_64 GNU/Linux インストール # apt-get install hugo バージョン $ hugo version Hugo Static Site Generator v0.54.0/extended linux/amd64 BuildDate: 2019-02-01T18:13:18Z ブログのような形式で情報が並ぶと目的の情報に辿り着くまでがしんどかったので、 タグも日時も表示されず、単一のページにタイトルだけがずらっと並び、 そこを Ctrl + f で検索するような、とにかくシンプルなテーマを探し、下記を見付けた。</description>
    </item>
    
    <item>
      <title></title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>@zinrai</description>
    </item>
    
  </channel>
</rss>
