<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>command not found:</title>
    <link>/</link>
    <description>Recent content on command not found:</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 12 Dec 2021 09:07:00 +0900</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prometheus の Exporter を自分なりに分類してみた</title>
      <link>/posts/classify-prometheus-exporter/</link>
      <pubDate>Sun, 12 Dec 2021 09:07:00 +0900</pubDate>
      
      <guid>/posts/classify-prometheus-exporter/</guid>
      <description>さくらインターネット Advent Calendar 2021 12日目の記事になります。
Prometheus を使うと何がうれしいのか自分なりに考えてみた にて、 Prometheus ではどのようなモニタリングを実現でき、従来のモニタリングの仕組みと比べてのうれしい部分について考えてみました。
Prometheus では、 Exporter を使い対象のメトリクスを取得するということを説明しました。 Exporter がどのようなものであるかを説明したあと、 すぐに ○○ Exporter の使い方というような流れであったり、 特定の Exporter の使い方という記事はよく見掛けます。
対象のメトリクスを取得するために、 どのような Exporter の構成があるのかを説明した記事は見掛けたことが無く、 Prometheus と Exporter の結び付きをイメージするためには必要なのではないかと常日頃から思っていたので、 そこに対する自分の考えを書いてみます。
Exporter の構成 対象のメトリクスを取得するためにとる Exporter の構成は、下記の3つになるのではないかと考えています。 Sidecar exporter, Native exporter, Proxy exporter という名前は、 説明のために、私が作ったものであり、 Prometheus で使われている一般的な用語ではありません。 Prometheus でメトリクスを管理するためには、 Prometheus の保存できる形式でメトリクスを生成し、 HTTP のエンドポイントを公開することが必須であるという前提のもと、 これらについて、どのような構成をとり、どう考え名前を付けたのかについて説明します。
Native exporter Apache のメトリクスを Prometheus で管理したいとなったときに、 Apache 自体は、 Exporter を持っていないため、 そのままでは、 Apache のメトリクスを Prometheus で管理できません。
Prometheus が保存できる形式でメトリクスを返すことができる機能を、 ソフトウェア自体に組み込んでいる Exporter を Native exporter と呼んでみました。</description>
    </item>
    
    <item>
      <title>Prometheus を使うと何がうれしいのか自分なりに考えてみた</title>
      <link>/posts/motivation-use-prometheus/</link>
      <pubDate>Sat, 11 Dec 2021 10:05:45 +0900</pubDate>
      
      <guid>/posts/motivation-use-prometheus/</guid>
      <description>さくらインターネット Advent Calendar 2021 11日目の記事になります。
Prometheus 以前のモニタリングの仕組みを考察してみる では、 Nagios, Cacti を例に Prometheus 以前のモニタリングの仕組みはどのようになっているかを考察し、 Nagios, Cacti で「さくらの専用サーバ PHY」のモニタリングを実装するとどうなりそうであるかを考え、 「けっこうアプリケーションを作ら作らなければならないな」ということが見えてきました。
今回は、 Prometheus を使うと、どのようなモニタリングの基盤を構成可能であるかを考察し、 Prometheus を使うと何がうれしいのかを考えてみます。
用語 Prometheus の話をするにあたり、知っておかなければ話を進められない用語について説明します。
メトリクス Prometheus は、下記のような形式にて対象の情報を取得し、メトリクスを呼ばれています。
メモリ利用量のメトリクスの例
# HELP node_memory_MemAvailable_bytes Memory information field MemAvailable_bytes. # TYPE node_memory_MemAvailable_bytes gauge node_memory_MemAvailable_bytes 6.2036496384e+10 Exporter エクスポーターと呼びます。 Prometheus にメトリクスを保存するためには、 Prometheus が保存できる形式のメトリクスを HTTP として公開する必要があります。 この機能を持ったアプリケーションを Exporter と呼んでいます。 Prometheus にメトリクスを保存するためには Exporter が必須となります。
Prometheus でのモニタリング Prometheus 以前のモニタリングの仕組みを考察してみる では、 Nagios, Cacti にて「さくらの専用サーバ PHY」のモニタリングを考察しました。 今回は、 Prometheus を使った場合の「さくらの専用サーバ PHY」のモニタリングを考察します。</description>
    </item>
    
    <item>
      <title>Prometheus 以前のモニタリングの仕組みを考察してみる</title>
      <link>/posts/monitoring-mechanism-before-prometheus/</link>
      <pubDate>Thu, 09 Dec 2021 13:32:10 +0900</pubDate>
      
      <guid>/posts/monitoring-mechanism-before-prometheus/</guid>
      <description>さくらインターネット Advent Calendar 2021 9日目の記事になります。
Prometheus を知っているでしょうか。 近年、モニタリングを賑わせているソフトウェアになります。
Prometheus についての内容、 Prometheus の設定方法、どのような Exporter があり、 どのように使うのかという記事は、インターネット上でたくさん目にしましたが、 従来のモニタリングの仕組みと比べ、 Prometheus がどのような点で異なっており、 その異なる部分がモニタリングにどのように生きてくるのかということを書いた記事は見掛けたことはありません。
自分なりに Prometheus 以前のモニタリングの仕組みはどうであるかを考察し、 Prometheus がイケてるという理由で使われているのではなく、 従来のモニタリングの仕組みと比べて、 Prometheus を使うと何がうれしいのかということを書いていけたらと思っています。
今回は、 Prometheus 以前のモニタリングの仕組みはどうであるか考察し、「さくらの専用サーバ PHY」に適用するとどうなりそうかについて考えてみます。
従来のモニタリングの仕組み モニタリングには、システムの異常に気が付き対応できるようにするための死活監視、 アラートが上がったときなどにシステムのリソースなどを時系列で確認するためのシステムが存在しているはずです。
下記の観点から従来のモニタリングの仕組みがどうであるかについて考察してみます。
 対象の追加・削除 ストレージ インタフェース スケールアウト  Nagios , Cacti を従来のモニタリングの仕組みとして考察します。
Nagios でのモニタリング Cacti でのモニタリング 対象の追加・削除 モニタリングしたい対象のサーバーが1台あったと仮定して、 モニタリングのため、死活監視を Nagios に登録し、グラフを Cacti に登録するなど、 それぞれのシステムごとにモニタリングの対象を登録していかなければなりません。
Nagios, Cacti ともに IPAM などと連携し対象を登録していくという仕組みはなく、 対象の追加・削除は手動で行うことになります。 「対象の追加・削除を自動化したい」となったときには、そのための仕組みを開発する必要がありそうです。
ストレージ 死活監視のデータは Nagios に保存され、 グラフのためのデータは Cacti に保存されるため、 システムごとにストレージが異っています。</description>
    </item>
    
    <item>
      <title>コンテナオーケストレーションにて必要になってくるもの</title>
      <link>/posts/technologies-required-for-container-orchestration/</link>
      <pubDate>Wed, 08 Dec 2021 08:05:12 +0900</pubDate>
      
      <guid>/posts/technologies-required-for-container-orchestration/</guid>
      <description>さくらインターネット Advent Calendar 2021 8日目の記事になります。
コンテナオーケストレーションを使うモチベーションを自分なりに考えてみた にて、 Docker を導入するだけでは解決しなかった課題を Nomad と組み合わせることにより解決できそうであるということを説明しました。
今回は、 Docker と Nomad を組み合わせるだけでは解決できていないことに対して必要になってくる技術について、 従来のシステム連携がどのようになっているかという話と絡めながら説明します。
Nomad を使うことで、 Docker を実行できるサーバーを沢山並べてアプリケーションを実行できるようになり、 アプリケーションとサーバーが密に結合しなくなりました。 Docker で動くアプリケーションが、どの Nomad Client で起動するのかは、 Nomad Server が管理しているため、人間にはコントロールできません。
アプリケーションは、他のアプリケーションと連携するというパターンが大部分を占めるのではないかと思います。 どこかの Nomad Client でアプリケーションが起動しているという状態で、 どのようにしてアプリケーションにアクセスすればよいでしょうか。
従来のシステム連携 Nomad Client で起動しているアプリケーションにどのようにアクセスするのかの手段を説明する前に、 従来のシステム連携がどうであるかについて、以下の観点から考えてみます。
 アプリケーションへの接続 依存関係  アプリケーションが連携している例になります。
アプリケーションへの接続 サーバーに割り当たっている IP アドレスからアプリケーションに接続するというやり方が考えられます。
IP アドレス以外の方法としては、 DNS を使いサーバーに割り当たっている IP アドレスに名前を付け、 アプリケーションに接続するというやり方が考えられます。
どちらのやり方の場合も、サーバーに割り当てられている IP アドレス、 名前からアプリケーションに接続するという構図になっているのではないかと思います。
システム連携の主体は、アプリケーションではなく、サーバーにあるように見えます。
依存関係 IP アドレス、 DNS などアプリケーションへの接続は、サーバーに紐付いている情報に依存しており、 アプリケーションに接続するというよりは、あるサーバー上で動いているアプリケーションに接続するという依存関係になっているように見えます。
従来のやり方でアプリケーションに接続できるか どれからの Nomad Client でアプリケーションが起動しているという状況にて、 従来のシステム連携のやり方が通用するのかについて考えてみます。</description>
    </item>
    
    <item>
      <title>コンテナオーケストレーションを使うモチベーションを自分なりに考えてみた</title>
      <link>/posts/motivation-using-container-orchestration/</link>
      <pubDate>Mon, 06 Dec 2021 10:26:12 +0900</pubDate>
      
      <guid>/posts/motivation-using-container-orchestration/</guid>
      <description>さくらインターネット Advent Calendar 2021 6日目の記事になります。
Docker でのアプリケーション実行環境を提供した状況を想像してみる にて、 Docker を導入しただけで、万事解決とはならなそうであるということを書きました。
今回は、 Docker の導入だけでは解決できないことに対して、どのようなことができればよいのかということを考え、それらを解決する具体的な手段について書いていきます。
Docker だけでは解決できていないこと 下記のようなことができれば、アプリケーション開発者はサーバーのことを気にせず、 サーバー管理者はアプリケーションのことを気にしない仕組みになるのではないかと考えました。
「 Docker を実行できる環境があれば、どこでもアプリケーションを実行できるが、サーバーで Docker を実行できるだけでは、サーバーに付けた名前や IP アドレスで、アプリケーションに接続するという構図から抜け出せない」
Docker を実行できるサーバーを沢山用意して、その中のどれかでアプリケーションが実行されるということができれば、 アプリケーションとサーバーが密に結合するという状態を解消できそうです。
「サーバー管理者の裁量でサーバーをコントロールできる部分が少なそう」
Docker を実行できるサーバーを沢山用意して、 その中のどれかでアプリケーションが実行されるということができるかつ、 メンテナンスしたいサーバーでは Docker コンテナを実行できないようにすれば、 アプリケーション開発者は、サーバーの停止を意識することがなくアプリケーションを実行でき、 サーバー管理者は、サーバーを任意のタイミングで停止できるようになりそうです。
「 Docker で動くアプリケーションをデプロイする統一されたインタフェースが無い」
定義ファイルを書いて実行すれば Docker で動くアプリケーションがデプロイされる仕組みがあれば、 構成管理もできてよさそうです。 アプリケーション開発者は、定義ファイルを書いて実行することでアプリケーションを実行し、 サーバー管理者は、 Ansible でサーバーを構成管理することで、 アプリケーションとサーバー管理の手段を分離できそうです。
Nomad を使えば実現できる Docker だけでは解決できていないことに対して、どのようなことができればよいかを考えてみました。 これらは Docker に Nomad というソフトウェアを組み合わせることで、実現できます。
「Nomad とは」というお話は、別の記事や公式ドキュメントに任せ、 Nomad と Docker を組み合わせることで何ができるのかについて、書いていきます。
https://www.nomadproject.io
アーキテクチャ https://learn.hashicorp.com/tutorials/nomad/production-reference-architecture-vm-with-consul#reference-diagram
緑色の Nomad と書かれているところだけを説明していきます。 ( 赤色の Consul と書かれているところは、今回は気にしないでください。 )</description>
    </item>
    
    <item>
      <title>Docker でのアプリケーション実行環境を提供した状況を想像してみる</title>
      <link>/posts/imagine-situation-using-docker/</link>
      <pubDate>Sun, 05 Dec 2021 13:45:07 +0900</pubDate>
      
      <guid>/posts/imagine-situation-using-docker/</guid>
      <description>さくらインターネット Advent Calendar 2021 5日目の記事になります。
Docker を使うと何がうれしいのか自分なりに考えてみた にて、 Docker を使うと、どういった面でうれしいことがあるのかについて書きました。
Docker を使うと何がうれしいのかということは説明できたかと思いますが、 Docker を提供すれば、「万事解決めでたしめでたし」となるのかというところを考えていきたいと思います。
サーバー管理者になって考えてみる 今回は、サーバー管理者として Docker でのアプリケーション実行環境をアプリケーション開発者に提供したと仮定し、 下記の観点にて、どのようなことが考えられそうかについて、説明していきます。
 依存関係 移植性 デプロイ  下記、 Docker によるバッチサーバー、アプリケーションサーバーを提供したとして、何が起きそうかについて想像力を働かせてみます。
バッチサーバー アプリケーションサーバー 依存関係 サーバーの中だけを見ると Docker が動く環境さえ用意すればアプリケーションはどこでも実行できる状態になっています。
アプリケーションサーバーは、どこかのアプリケーションからリクエストを受ける、システム連携の側面が強いのではないかと思います。
システム連携を考えたときに、どのようにアプリケーションに到達するのか考えてみます。 このサーバー構成では、サーバーに付けた名前 ( DNS ) や IP アドレスでアプリケーションに到達することになるでしょう。
このサーバーに付けた名前や IP アドレスで、アプリケーションに接続するというシステム連携になっており、 アプリケーションがサーバーとの依存関係を断てていないという状況が見えてきます。
Docker だけでは、アプリケーションがサーバーに強く依存している状況を打破することが難しそうだなということが考えられます。
移植性 一度、サーバー構築してしまえば、そこからはメンテナンスフリーとはならないことを皆さんよくご存知かと思います。 OS 、ミドルウェア、アプリケーションを更新していきながらサービスを維持していくことになるはずです。 Docker も Linux などのサーバー上で動くため、これらの更新からは逃れられません。
バッチサーバーの OS にパッチを当て再起動するという状況は容易に想像できるかと思います。 アプリケーション開発者にバッチサーバーを再起動しますと調整するコストが発生しそうです。
バッチサーバー、アプリケーションサーバーの OS を更新するという作業も容易に想像できるかと思います。 アプリケーション開発者と OS を更新するための計画を調整するコストが発生しそうです。
サーバーの中だけを見ると Docker が動く環境さえ用意すればアプリケーションはどこでも実行できる状態になっていますが、 アプリケーションとサーバーが密に結合しており、サーバー管理者の裁量でサーバーをコントロールできる部分が少なそうに思います。</description>
    </item>
    
    <item>
      <title>Docker を使うと何がうれしいのか自分なりに考えてみた</title>
      <link>/posts/benefits-of-using-docker/</link>
      <pubDate>Sat, 04 Dec 2021 08:37:56 +0900</pubDate>
      
      <guid>/posts/benefits-of-using-docker/</guid>
      <description>さくらインターネット Advent Calendar 2021 4日目の記事になります。
Docker が何者なのか、 Docker の使い方などの情報はインターネットに沢山公開されていますが、 「 Docker を使うと何がうれしいのか」という情報が目に入ってきたことはないので、 サーバー管理者、アプリケーション開発者がいる環境で、 Docker を使うと何がうれしいのかということを自分なりに考え、テキストに起こしてみました。
きっかけ 文学フリマに出店したときに、「インフラエンジニアやってます。仕事は仮想サーバーですが、 Docker に興味はあって調べてはみたのですが、 Docker を使うと何がうれしいのかということをイメージするには至れなかったです。仮想サーバーから Docker になると何がうれしいのかということをイメージできるような情報があると、 このモヤモヤが晴れるのではないかと思っています。」という話を参加者の一人からされました。
確かに、自分もそのような情報が目に入ってきたことはりませんでした。 社内で「 Docker を使うと何がうれしいのか」というテキストを自分で書いて、チーム内に展開しており、 会社の情報は入っていないので、これを公開すればよいかもしれないなと考え、今回、この記事を書くに至っています。
アプリケーションとランタイム これから先の説明に利用する、アプリケーションとランライムという言葉を定義しておきます。
 アプリケーション: 目的の処理を実行するもの ランタイム: アプリケーションを実行するときに必要なもの  シェルスクリプトで、集計するスクリプトを作成したと仮定した場合に、 作成したシェルスクリプトはアプリケーションで、 シェルスクリプトの中で sed, awk などを使っていれば、 シェルスクリプトというアプリケーションの実行に必要なものということで、 sed, awk はランタイムとなります。
想定読者 下記に該当する方に自分ごととしてイメージしながら読んでもらえるのではないかと考えています。
 アプリケーションを実行するための環境を Linux でサーバーを構築している サーバーを管理する人とアプリケーションを開発する人が分かれており、アプリケーションを実行するための環境を Linux で構築するサーバーを管理する側の人である バッチサーバーなるサーバーを用意し、そこで cron を使い様々なバッチを実行している  Docker は何をするものなのか インターネットに沢山の情報があるため、 Docker の詳細な内容は、そちらにお任せします。
 アプリケーションを実行する手段 アプリケーションを Docker の提供している方法でパッケージングする Docker の提供している方法でパッケージングした(された)アプリケーションを Docker の提供している方法で実行する Docker の提供している方法でパッケージングした(された)アプリケーションを実行するため、サーバーに Docker がインストールする  社内で動かしている Redmine のアクティビティを定期的に Slack へ通知するアプリケーションを例に、 サーバー上でのアプリケーション、ランタイムの構成を図にしました。</description>
    </item>
    
    <item>
      <title>Promxy を使ったアーキテクチャ</title>
      <link>/posts/promxy-architecture/</link>
      <pubDate>Fri, 11 Dec 2020 07:18:38 +0900</pubDate>
      
      <guid>/posts/promxy-architecture/</guid>
      <description>さくらインターネット Advent Calendar 2020 11日目の記事になります。
「さくらの専用サーバ PHY」にてメトリクス保存・公開の基盤を担当している者です。 アプリケーション、サーバー、ネットワーク機器などのメトリクスを保存・公開する基盤として Prometheus を利用しています。 Prometheus にあるメトリクスを使い、 ユーザーにトラフィックグラフを提供したり、 自分達がシステムの異常に気が付くための監視を構成しています。
Prometheus のメトリクスを利用するにあたり、抱えていた2つの課題を Promxy を利用することによって解決することができ、 そこに至るまで、どのように考え、 どのようなアーキテクチャをとったのか、これからどうしていこうと考えているのかについて書きました。
ユーザーにトラフィックグラフを提供する 「さくらの専用サーバ PHY」のコントロールパネルにてトラフィックグラフを提供しており、 値は Prometheus に保存されているメトリクスを使っています。 ユーザーにトラフィックグラフのサービスを提供するにあたり、 インフラ側の可用性を上げるための構成を考え、実装する必要がありました。
Prometheus での高可用性 2台の Prometheus を動かし同じメトリクスを取得させるというのが公式の見解となっています。
https://prometheus.io/docs/introduction/faq/#can-prometheus-be-made-highly-available
ロードバランサを使った構成 Prometheus 公式の構成で、ロードバランサをエンドポイントにしてバックエンドに2台の Prometheus がいるという構成をとったとして、どうなるかを考えてみました。
この構成では、どちらからの Prometheus が一定時間落ちて復帰した場合、 保存しているメトリクスの対称性が失われてしまい、 エンドポイントにリクエストするごとに返ってくるメトリクスの結果が異なってしまうという状態がが発生するなと考え、採用を見送りました。
ロードバランサと Promxy を組み合わせた構成 Promxy は、メトリクスを Aggregate する機能を持っています。 Prometheus のセットが同じタイミングで落ちない限りは、 対称性の失われた Prometheus のセットに対して、 欠損することなくメトリクスを返すことができるため、 Promxy を使ったこちらの構成を採用しました。
アラートルールの管理 サービスの利用者が増えれば、利用される機器も増え、取得するメトリクスも線形に増加していきます。 1台の Prometheus 取得できるメトリクスには限界があり、 「さくらの専用サーバ PHY」で扱っている全てのメトリクスは管理できません。 Prometheus を複数台運用することが前提となります。 Prometheus のメトリクスを使ったアラート発報のためのアラートルールをどのように管理していくのかという問題に当たりました。</description>
    </item>
    
    <item>
      <title>cowbuilder</title>
      <link>/posts/cowbuilder/</link>
      <pubDate>Wed, 30 Sep 2020 00:27:00 +0900</pubDate>
      
      <guid>/posts/cowbuilder/</guid>
      <description>仕事で様々なバージョンの Debian パッケージを作らなければならなかったとき、 環境を用意するコストがとても低くて重宝した。
% apt-get install cowbuilder chroot 環境の作成
% cowbuilder --create --distribution buster --architecture amd64 --basepath /var/cache/pbuilder/buster64.cow chroot 環境へのログイン
# cowbuilder --login --basepath /var/cache/pbuilder/buster64.cow &amp;ndash;save オプションを付けると chroot 環境への変更を保存できる。
% cowbuilder --login --save --basepath /var/cache/pbuilder/buster64.cow bind mount
% cowbuilder --login --save --bindmount /home/workdir --basepath /var/cache/pbuilder/buster64.cow 下記を実行し、作成した chroot 環境をまとめて更新していた。
% for a in /var/cache/pbuilder/*.cow; do cowbuilder --update --basepath $a; done </description>
    </item>
    
    <item>
      <title>LXC</title>
      <link>/posts/lxc/</link>
      <pubDate>Tue, 29 Sep 2020 17:50:53 +0900</pubDate>
      
      <guid>/posts/lxc/</guid>
      <description>$ uname -srv Linux 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) $ cat /etc/debian_version 10.4 依存関係でいろいろ入ってしまうけれども iptables, dnsmasq, bridge-utils それぞれを操作するのが面倒なので、 virsh (1) で全てよしなにやってもらうことにした。
% apt-get install libvirt-clients libvirt-daemon-system dnsmasq bridge-utils % virsh net-list Name State Autostart Persistent -------------------------------------------- default active no yes $ ip addr show virbr0 3: virbr0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 52:54:00:75:2f:a9 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever LXC インストール</description>
    </item>
    
    <item>
      <title>chroot &#43; OverlayFS</title>
      <link>/posts/chroot-overlayfs/</link>
      <pubDate>Mon, 28 Sep 2020 22:59:31 +0900</pubDate>
      
      <guid>/posts/chroot-overlayfs/</guid>
      <description>出番がないかもしれないが、 OverlayFS で chroot 環境をクリーンに保つをやってみた。
$ uname -srv Linux 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) $ cat /etc/debian_version 10.4 % debootstrap buster ./buster64 http://ftp.jp.debian.org/debian OverlayFS でのマウントについては mount (8) を参照
% mkdir upper workdir % mount -t overlay overlay -o lowerdir=./buster64,upperdir=./upper,workdir=./workdir ./buster64 % df ファイルシス 1K-ブロック 使用 使用可 使用% マウント位置 udev 2005984 0 2005984 0% /dev tmpfs 404160 5436 398724 2% /run /dev/vda3 99016264 2958444 91008396 4% / tmpfs 2020792 0 2020792 0% /dev/shm tmpfs 5120 0 5120 0% /run/lock tmpfs 2020792 0 2020792 0% /sys/fs/cgroup overlay 99016264 2958444 91008396 4% /root/buster64 % mount --rbind /dev .</description>
    </item>
    
    <item>
      <title>OpenConnect</title>
      <link>/posts/openconnect/</link>
      <pubDate>Thu, 04 Apr 2019 01:27:45 +0900</pubDate>
      
      <guid>/posts/openconnect/</guid>
      <description>GlobalProtect で VPN 接続する機会があり、 Mac も Windows も持っていないのでどうしたものかと調べていたところ、 OpenConnect というソフトウェアで GlobalProtect に VPN 接続することができることを知ったので記録として残しておく。 OpenConnect の開発者と Debian パッケージのメンテナーありがとう。
openconnect
https://packages.debian.org/ja/sid/openconnect
環境 $ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ uname -a Linux koyomi 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15) x86_64 GNU/Linux fchu@koyomi:~$ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ apt-cache show openconnect Package: openconnect Version: 8.</description>
    </item>
    
    <item>
      <title>HUGO</title>
      <link>/posts/hugo/</link>
      <pubDate>Sat, 19 Jan 2019 21:47:42 +0900</pubDate>
      
      <guid>/posts/hugo/</guid>
      <description>debian-devel-changes のメーリングリストを眺めていたら、 HUGO という Site Generator を見付けた。 数年前にブログ環境を消失してから、ブログを書く気力が無くなってしまっていたが、HUGO を使い Github Pages にブログを書くことを再開しようと思う。
debian-devel-changes
https://lists.debian.org/debian-devel-changes/
HUGO
https://gohugo.io/
HUGO の Debian パッケージ
https://packages.debian.org/sid/hugo
実行環境 $ lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux buster/sid Release: unstable Codename: sid $ uname -a Linux koyomi 4.19.0-4-amd64 #1 SMP Debian 4.19.28-2 (2019-03-15) x86_64 GNU/Linux インストール # apt-get install hugo バージョン $ hugo version Hugo Static Site Generator v0.54.0/extended linux/amd64 BuildDate: 2019-02-01T18:13:18Z ブログのような形式で情報が並ぶと目的の情報に辿り着くまでがしんどかったので、 タグも日時も表示されず、単一のページにタイトルだけがずらっと並び、 そこを Ctrl + f で検索するような、とにかくシンプルなテーマを探し、下記を見付けた。</description>
    </item>
    
    <item>
      <title></title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>@zinrai</description>
    </item>
    
  </channel>
</rss>
